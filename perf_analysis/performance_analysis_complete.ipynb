{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8c3e1b0",
   "metadata": {},
   "source": [
    "# Performance Analysis Framework\n",
    "\n",
    "This notebook provides a comprehensive framework for analyzing model performance against statistical baselines using recall metrics and confidence intervals.\n",
    "\n",
    "## Key Features:\n",
    "- **Statistical Significance Testing**: Compare model performance against random baseline\n",
    "- **Confidence Intervals**: Calculate Wilson confidence intervals for recall metrics\n",
    "- **Comprehensive Visualizations**: Multiple plots for performance analysis\n",
    "- **Theoretical Validation**: Simulation-based validation of analytical methods\n",
    "\n",
    "## Signal Convention:\n",
    "- **-1**: Sell signal\n",
    "- **0**: Hold signal  \n",
    "- **1**: Buy signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ef3e290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Performance Analysis Framework Loaded Successfully!\n",
      "ðŸŽ¯ Ready for statistical significance testing and visualization\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ðŸ“Š Performance Analysis Framework Loaded Successfully!\")\n",
    "print(\"ðŸŽ¯ Ready for statistical significance testing and visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c08e6a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = pd.read_parquet(\"../data/features/DATA_1/ETH_EUR.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407084cf",
   "metadata": {},
   "source": [
    "## 1. Core Recall Calculation Functions\n",
    "\n",
    "These functions form the foundation of our performance analysis framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e52f966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Recall calculation function defined\n",
      "ðŸ“ Formula: Recall = TP / (TP + FN)\n"
     ]
    }
   ],
   "source": [
    "def recall(predictions: pd.Series, targets: pd.Series, signal: int) -> float:\n",
    "    \"\"\"\n",
    "    Calculate recall performance metric for a specific signal.\n",
    "    \n",
    "    Recall = True Positives / (True Positives + False Negatives)\n",
    "    \n",
    "    Args:\n",
    "        predictions (pd.Series): Predicted values (-1, 0, 1)\n",
    "        targets (pd.Series): True target values (-1, 0, 1)\n",
    "        signal (int): The signal value to calculate recall for\n",
    "        \n",
    "    Returns:\n",
    "        float: Recall performance metric (0.0 to 1.0)\n",
    "    \"\"\"\n",
    "    true_positives = np.sum((predictions == signal) & (targets == signal))\n",
    "    all_positives = np.sum(targets == signal)\n",
    "    \n",
    "    if all_positives == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return true_positives / all_positives\n",
    "\n",
    "# Test the recall function\n",
    "print(\"âœ… Recall calculation function defined\")\n",
    "print(\"ðŸ“ Formula: Recall = TP / (TP + FN)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daffc9c",
   "metadata": {},
   "source": [
    "## 2. Random Baseline Confidence Intervals\n",
    "\n",
    "Calculate confidence intervals for recall under random baseline prediction using Wilson method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49595575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'signal-1': (0.0, 0.0), 'signal0': (0.9237211439928714, 0.924341508382823), 'signal1': (0.07489335434610084, 0.0770569690326472)}\n"
     ]
    }
   ],
   "source": [
    "def recall_interval_random_baseline(targets: pd.Series, signal: int, confidence: float = 0.95) -> tuple:\n",
    "    \"\"\"\n",
    "    Calculate the confidence interval for recall under random baseline prediction.\n",
    "    \n",
    "    For a random predictor that follows the target distribution, the expected recall \n",
    "    for each signal equals the proportion of that signal in the targets.\n",
    "    \n",
    "    Args:\n",
    "        targets (pd.Series): True target values\n",
    "        signal (int): The signal value (-1, 0, or 1) to calculate recall for\n",
    "        confidence (float): Confidence level for the interval (default: 0.95)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (lower_bound, upper_bound) of the confidence interval\n",
    "    \"\"\"\n",
    "    total_samples = len(targets)\n",
    "    actual_positives = np.sum(targets == signal)\n",
    "    \n",
    "    if actual_positives == 0:\n",
    "        return 0.0, 0.0\n",
    "    \n",
    "    # Under random prediction matching target proportions:\n",
    "    # P(predict signal) = proportion of signal in targets\n",
    "    # Expected recall = P(predict signal | true signal) = P(predict signal) = proportion\n",
    "    signal_proportion = actual_positives / total_samples\n",
    "    \n",
    "    # Expected true positives under random prediction\n",
    "    expected_tp = actual_positives * signal_proportion\n",
    "    \n",
    "    # Use Wilson confidence interval (more robust than normal approximation)\n",
    "    lower, upper = proportion_confint(\n",
    "        count=expected_tp,\n",
    "        nobs=actual_positives,\n",
    "        alpha=1 - confidence,\n",
    "        method='wilson'\n",
    "    )\n",
    "    \n",
    "    return lower, upper\n",
    "\n",
    "target = target_feature[\"avg-10ms-of-mid-price-itincreases-after-200ms-with-threshold-5\"]\n",
    "interval = {f\"signal{i}\": recall_interval_random_baseline(target, signal=i) for i in [-1, 0, 1]}\n",
    "print(interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8a7e1e",
   "metadata": {},
   "source": [
    "## 3. Comprehensive Baseline Analysis\n",
    "\n",
    "Compute confidence intervals for all signals with detailed statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c8ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "import pandas as pd\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "def compute_all_recall_intervals_random_baseline_from_recalls(recall_values: Dict[int, float], \n",
    "                                                                  signal_counts: Dict[int, int],\n",
    "                                                                  confidence: float = 0.95, \n",
    "                                                                  verbose: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Compute confidence intervals for recall of all signals under random baseline,\n",
    "    working directly with recall values instead of predictions/targets.\n",
    "    \n",
    "    Args:\n",
    "        recall_values (Dict[int, float]): Dictionary mapping signal -> actual recall value\n",
    "        signal_counts (Dict[int, int]): Dictionary mapping signal -> count of occurrences\n",
    "        confidence (float): Confidence level for the interval\n",
    "        verbose (bool): Whether to print detailed results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing recall intervals for each signal and summary statistics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    total_samples = sum(signal_counts.values())\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"ðŸŽ¯ RANDOM BASELINE RECALL CONFIDENCE INTERVALS (FROM RECALL VALUES)\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Total samples: {total_samples:,}\")\n",
    "        print(f\"Confidence level: {confidence*100:.1f}%\")\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    # Calculate for each signal\n",
    "    for signal in sorted(signal_counts.keys()):\n",
    "        count = signal_counts[signal]\n",
    "        proportion = count / total_samples\n",
    "        \n",
    "        # Expected recall under random prediction = signal proportion\n",
    "        expected_recall = proportion\n",
    "        \n",
    "        # Confidence interval using Wilson method\n",
    "        # Expected true positives under random prediction\n",
    "        expected_tp = count * proportion\n",
    "        \n",
    "        # Use Wilson confidence interval\n",
    "        lower, upper = proportion_confint(\n",
    "            count=expected_tp,\n",
    "            nobs=count,\n",
    "            alpha=1 - confidence,\n",
    "            method='wilson'\n",
    "        )\n",
    "        \n",
    "        results[signal] = {\n",
    "            'count': count,\n",
    "            'proportion': proportion,\n",
    "            'expected_recall': expected_recall,\n",
    "            'ci_lower': lower,\n",
    "            'ci_upper': upper,\n",
    "            'ci_width': upper - lower,\n",
    "            'actual_recall': recall_values.get(signal, 0.0)  # Store actual recall for comparison\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            actual_recall = recall_values.get(signal, 0.0)\n",
    "            print(f\"Signal {signal:2d}: count={count:6,} ({proportion:.3f}) | \"\n",
    "                  f\"Actual recall={actual_recall:.3f} | \"\n",
    "                  f\"Expected recall={expected_recall:.3f} | \"\n",
    "                  f\"CI=[{lower:.3f}, {upper:.3f}] | width={upper-lower:.3f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{'='*70}\")\n",
    "    \n",
    "    # Add summary statistics\n",
    "    results['summary'] = {\n",
    "        'total_samples': total_samples,\n",
    "        'unique_signals': len(signal_counts),\n",
    "        'most_frequent_signal': max(signal_counts.keys(), key=signal_counts.get),\n",
    "        'most_frequent_proportion': max(signal_counts.values()) / total_samples,\n",
    "        'least_frequent_signal': min(signal_counts.keys(), key=signal_counts.get),\n",
    "        'least_frequent_proportion': min(signal_counts.values()) / total_samples,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Keep original function for backward compatibility\n",
    "def compute_all_recall_intervals_random_baseline(targets: pd.Series, confidence: float = 0.95, verbose: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Original function: Compute confidence intervals for recall of all signals under random baseline.\n",
    "    \n",
    "    Args:\n",
    "        targets (pd.Series): True target values containing -1, 0, 1\n",
    "        confidence (float): Confidence level for the interval\n",
    "        verbose (bool): Whether to print detailed results\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing recall intervals for each signal and summary statistics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    signal_counts = targets.value_counts()\n",
    "    total_samples = len(targets)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"ðŸŽ¯ RANDOM BASELINE RECALL CONFIDENCE INTERVALS\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Total samples: {total_samples:,}\")\n",
    "        print(f\"Confidence level: {confidence*100:.1f}%\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    # Calculate for each signal present in targets\n",
    "    for signal in sorted(signal_counts.index):\n",
    "        count = signal_counts[signal]\n",
    "        proportion = count / total_samples\n",
    "        \n",
    "        # Expected recall under random prediction = signal proportion\n",
    "        expected_recall = proportion\n",
    "        \n",
    "        # Confidence interval\n",
    "        lower, upper = recall_interval_random_baseline(targets, signal, confidence)\n",
    "        \n",
    "        results[signal] = {\n",
    "            'count': count,\n",
    "            'proportion': proportion,\n",
    "            'expected_recall': expected_recall,\n",
    "            'ci_lower': lower,\n",
    "            'ci_upper': upper,\n",
    "            'ci_width': upper - lower\n",
    "        }\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Signal {signal:2d}: count={count:6,} ({proportion:.3f}) | \"\n",
    "                  f\"Expected recall={expected_recall:.3f} | \"\n",
    "                  f\"CI=[{lower:.3f}, {upper:.3f}] | width={upper-lower:.3f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    # Add summary statistics\n",
    "    results['summary'] = {\n",
    "        'total_samples': total_samples,\n",
    "        'unique_signals': len(signal_counts),\n",
    "        'most_frequent_signal': signal_counts.idxmax(),\n",
    "        'most_frequent_proportion': signal_counts.max() / total_samples,\n",
    "        'least_frequent_signal': signal_counts.idxmin(),\n",
    "        'least_frequent_proportion': signal_counts.min() / total_samples,\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Enhanced baseline analysis functions defined\")\n",
    "print(\"ðŸ“Š Now supports both direct recall input and traditional prediction/target input\")\n",
    "print(\"ðŸŽ¯ Use compute_all_recall_intervals_random_baseline_from_recalls() for direct recall analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183135c1",
   "metadata": {},
   "source": [
    "## 4. Statistical Significance Testing\n",
    "\n",
    "The main function to test if model performance is significantly better than random baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db17de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_recall_significance_from_recalls(recall_values: Dict[int, float], \n",
    "                                                signal_counts: Dict[int, int],\n",
    "                                                confidence: float = 0.95) -> dict:\n",
    "    \"\"\"\n",
    "    ðŸŽ¯ MAIN SIGNIFICANCE TESTING FUNCTION (FROM RECALL VALUES)\n",
    "    \n",
    "    Check if recall values are significantly better than random baseline for all signals.\n",
    "    \n",
    "    Args:\n",
    "        recall_values (Dict[int, float]): Dictionary mapping signal -> actual recall value\n",
    "        signal_counts (Dict[int, int]): Dictionary mapping signal -> count of occurrences  \n",
    "        confidence (float): Confidence level for the significance test\n",
    "        \n",
    "    Returns:\n",
    "        dict: Significance test results for each signal including:\n",
    "              - significant: bool (True if significantly better than random)\n",
    "              - recall: float (actual recall)\n",
    "              - ci_lower/ci_upper: float (random baseline confidence interval)\n",
    "              - expected_random: float (expected recall under random prediction)\n",
    "    \"\"\"\n",
    "    # Get random baseline confidence intervals using the new function\n",
    "    ci_results = compute_all_recall_intervals_random_baseline_from_recalls(\n",
    "        recall_values, signal_counts, confidence\n",
    "    )\n",
    "    \n",
    "    # Perform significance test: actual recall > upper bound of random CI\n",
    "    return {signal: {\n",
    "        \"significant\": recall_values[signal] > ci_results[signal][\"ci_upper\"],\n",
    "        'recall': recall_values[signal],\n",
    "        'ci_lower': ci_results[signal][\"ci_lower\"],\n",
    "        'ci_upper': ci_results[signal][\"ci_upper\"],\n",
    "        'expected_random': ci_results[signal][\"expected_recall\"]\n",
    "    } for signal in recall_values.keys() if signal in ci_results}\n",
    "\n",
    "# Keep original function for backward compatibility\n",
    "def prediction_recall_significance(predictions: pd.Series, targets: pd.Series, confidence: float = 0.95) -> dict:\n",
    "    \"\"\"\n",
    "    ðŸŽ¯ MAIN SIGNIFICANCE TESTING FUNCTION (ORIGINAL)\n",
    "    \n",
    "    Check if predictions recall is significantly better than random baseline for all signals.\n",
    "    \n",
    "    Args:\n",
    "        predictions (pd.Series): Predicted values (-1, 0, 1)\n",
    "        targets (pd.Series): True target values (-1, 0, 1)\n",
    "        confidence (float): Confidence level for the significance test\n",
    "        \n",
    "    Returns:\n",
    "        dict: Significance test results for each signal including:\n",
    "              - significant: bool (True if significantly better than random)\n",
    "              - recall: float (actual recall)\n",
    "              - ci_lower/ci_upper: float (random baseline confidence interval)\n",
    "              - expected_random: float (expected recall under random prediction)\n",
    "    \"\"\"\n",
    "    # Calculate actual recall for each signal\n",
    "    recall_values = {signal: recall(predictions, targets, signal) for signal in [-1, 0, 1]}\n",
    "    \n",
    "    # Get random baseline confidence intervals\n",
    "    ci_results = compute_all_recall_intervals_random_baseline(targets, confidence)\n",
    "    \n",
    "    # Perform significance test: actual recall > upper bound of random CI\n",
    "    return {signal: {\n",
    "        \"significant\": recall_values[signal] > ci_results[signal][\"ci_upper\"],\n",
    "        'recall': recall_values[signal],\n",
    "        'ci_lower': ci_results[signal][\"ci_lower\"],\n",
    "        'ci_upper': ci_results[signal][\"ci_upper\"],\n",
    "        'expected_random': ci_results[signal][\"expected_recall\"]\n",
    "    } for signal in [-1, 0, 1] if signal in ci_results}\n",
    "\n",
    "print(\"ðŸŽ¯ ENHANCED SIGNIFICANCE TESTING FUNCTIONS DEFINED\")\n",
    "print(\"âœ… Use prediction_recall_significance_from_recalls() for direct recall analysis!\")\n",
    "print(\"ðŸ“Š Returns complete significance analysis for all signals\")\n",
    "print(\"ðŸ”„ Original function preserved for backward compatibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e17734",
   "metadata": {},
   "source": [
    "## 5. Visualization Functions\n",
    "\n",
    "### 5.1 Random Baseline Confidence Intervals Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1498319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_recall_confidence_intervals(targets: pd.Series, confidence: float = 0.95, \n",
    "                                   figsize: Tuple[int, int] = (12, 8), \n",
    "                                   title_prefix: str = \"\") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Visualize recall confidence intervals for random baseline prediction.\n",
    "    \n",
    "    Args:\n",
    "        targets (pd.Series): True target values containing -1, 0, 1\n",
    "        confidence (float): Confidence level for the interval\n",
    "        figsize (tuple): Figure size (width, height)\n",
    "        title_prefix (str): Prefix for the plot title\n",
    "        \n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib figure object\n",
    "    \"\"\"\n",
    "    results = compute_all_recall_intervals_random_baseline(targets, confidence)\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)\n",
    "    fig.suptitle(f'{title_prefix}Random Baseline Recall Analysis (CI: {confidence*100:.1f}%)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    signals = [-1, 0, 1]\n",
    "    signal_names = ['Sell (-1)', 'Hold (0)', 'Buy (1)']\n",
    "    colors = ['red', 'gray', 'green']\n",
    "    \n",
    "    expected_recalls = [results[signal]['expected_recall'] for signal in signals if signal in results]\n",
    "    ci_lowers = [results[signal]['ci_lower'] for signal in signals if signal in results]\n",
    "    ci_uppers = [results[signal]['ci_upper'] for signal in signals if signal in results]\n",
    "    counts = [results[signal]['count'] for signal in signals if signal in results]\n",
    "    present_signals = [signal for signal in signals if signal in results]\n",
    "    present_names = [signal_names[i] for i, signal in enumerate(signals) if signal in results]\n",
    "    present_colors = [colors[i] for i, signal in enumerate(signals) if signal in results]\n",
    "    \n",
    "    # Plot 1: Expected Recalls with Confidence Intervals\n",
    "    ax1.bar(present_names, expected_recalls, color=present_colors, alpha=0.7, edgecolor='black')\n",
    "    ax1.errorbar(present_names, expected_recalls, \n",
    "                yerr=[np.array(expected_recalls) - np.array(ci_lowers),\n",
    "                      np.array(ci_uppers) - np.array(expected_recalls)],\n",
    "                fmt='none', color='black', capsize=5, capthick=2)\n",
    "    ax1.set_title('Expected Recall with Confidence Intervals', fontweight='bold')\n",
    "    ax1.set_ylabel('Recall')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_ylim(0, max(ci_uppers) * 1.1 if ci_uppers else 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (recall, ci_lower, ci_upper) in enumerate(zip(expected_recalls, ci_lowers, ci_uppers)):\n",
    "        ax1.text(i, recall + 0.01, f'{recall:.3f}\\n[{ci_lower:.3f}, {ci_upper:.3f}]', \n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Signal Distribution\n",
    "    ax2.pie(counts, labels=[f'{name}\\n({count:,})' for name, count in zip(present_names, counts)], \n",
    "            colors=present_colors, autopct='%1.2f%%', startangle=90)\n",
    "    ax2.set_title('Signal Distribution', fontweight='bold')\n",
    "    \n",
    "    # Plot 3: Confidence Interval Widths\n",
    "    ci_widths = [upper - lower for lower, upper in zip(ci_lowers, ci_uppers)]\n",
    "    bars = ax3.bar(present_names, ci_widths, color=present_colors, alpha=0.7, edgecolor='black')\n",
    "    ax3.set_title('Confidence Interval Widths', fontweight='bold')\n",
    "    ax3.set_ylabel('CI Width')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, width in zip(bars, ci_widths):\n",
    "        height = bar.get_height()\n",
    "        ax3.text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                f'{width:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Plot 4: Summary Statistics\n",
    "    ax4.axis('off')\n",
    "    summary_text = f\"\"\"\n",
    "    ðŸ“Š SUMMARY STATISTICS\n",
    "    {'='*30}\n",
    "    Total Samples: {results['summary']['total_samples']:,}\n",
    "    Unique Signals: {results['summary']['unique_signals']}\n",
    "    \n",
    "    Most Frequent Signal: {results['summary']['most_frequent_signal']} \n",
    "    ({results['summary']['most_frequent_proportion']:.3f})\n",
    "    \n",
    "    Least Frequent Signal: {results['summary']['least_frequent_signal']} \n",
    "    ({results['summary']['least_frequent_proportion']:.3f})\n",
    "    \n",
    "    Confidence Level: {confidence*100:.1f}%\n",
    "    \n",
    "    ðŸŽ¯ Under random baseline:\n",
    "    Expected Recall = Signal Proportion\n",
    "    \"\"\"\n",
    "    ax4.text(0.05, 0.95, summary_text, transform=ax4.transAxes, fontsize=11,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"ðŸ“Š Random baseline visualization function defined\")\n",
    "print(\"âœ… Creates 4-panel plot showing confidence intervals and statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bbd1f9",
   "metadata": {},
   "source": [
    "### 5.2 Prediction Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d9cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_performance_from_recalls(recall_values: Dict[int, float], \n",
    "                                           signal_counts: Dict[int, int],\n",
    "                                           confidence: float = 0.95, \n",
    "                                           figsize: Tuple[int, int] = (15, 10),\n",
    "                                           title_prefix: str = \"\") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    ðŸŽ¯ COMPREHENSIVE PERFORMANCE VISUALIZATION (FROM RECALL VALUES)\n",
    "    \n",
    "    Visualize recall performance against random baseline with significance testing,\n",
    "    working directly with recall values instead of predictions/targets.\n",
    "    \n",
    "    Args:\n",
    "        recall_values (Dict[int, float]): Dictionary mapping signal -> actual recall value\n",
    "        signal_counts (Dict[int, int]): Dictionary mapping signal -> count of occurrences\n",
    "        confidence (float): Confidence level for significance testing\n",
    "        figsize (tuple): Figure size (width, height)\n",
    "        title_prefix (str): Prefix for the plot title\n",
    "        \n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib figure object with 4 subplots\n",
    "    \"\"\"\n",
    "    # Get significance results\n",
    "    significance_results = prediction_recall_significance_from_recalls(\n",
    "        recall_values, signal_counts, confidence\n",
    "    )\n",
    "    baseline_results = compute_all_recall_intervals_random_baseline_from_recalls(\n",
    "        recall_values, signal_counts, confidence\n",
    "    )\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)\n",
    "    fig.suptitle(f'{title_prefix}Recall Performance vs Random Baseline', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Prepare data\n",
    "    signals = [-1, 0, 1]\n",
    "    signal_names = ['Sell (-1)', 'Hold (0)', 'Buy (1)']\n",
    "    colors = ['red', 'gray', 'green']\n",
    "    \n",
    "    # Filter for present signals\n",
    "    present_signals = [s for s in signals if s in significance_results]\n",
    "    present_names = [signal_names[signals.index(s)] for s in present_signals]\n",
    "    present_colors = [colors[signals.index(s)] for s in present_signals]\n",
    "    \n",
    "    actual_recalls = [significance_results[signal]['recall'] for signal in present_signals]\n",
    "    expected_recalls = [significance_results[signal]['expected_random'] for signal in present_signals]\n",
    "    ci_lowers = [significance_results[signal]['ci_lower'] for signal in present_signals]\n",
    "    ci_uppers = [significance_results[signal]['ci_upper'] for signal in present_signals]\n",
    "    is_significant = [significance_results[signal]['significant'] for signal in present_signals]\n",
    "    \n",
    "    # Plot 1: Actual vs Expected Recall Comparison\n",
    "    x_pos = np.arange(len(present_signals))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x_pos - width/2, actual_recalls, width, label='Actual Recall', \n",
    "                    color=present_colors, alpha=0.8, edgecolor='black')\n",
    "    bars2 = ax1.bar(x_pos + width/2, expected_recalls, width, label='Expected (Random)', \n",
    "                    color=present_colors, alpha=0.4, edgecolor='black', hatch='///')\n",
    "    \n",
    "    # Add confidence intervals for expected recalls\n",
    "    ax1.errorbar(x_pos + width/2, expected_recalls, \n",
    "                yerr=[np.array(expected_recalls) - np.array(ci_lowers),\n",
    "                      np.array(ci_uppers) - np.array(expected_recalls)],\n",
    "                fmt='none', color='black', capsize=3, capthick=1)\n",
    "    \n",
    "    ax1.set_title('Actual vs Expected Recall', fontweight='bold')\n",
    "    ax1.set_ylabel('Recall')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(present_names)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add significance markers\n",
    "    for i, (actual, significant) in enumerate(zip(actual_recalls, is_significant)):\n",
    "        marker = 'â˜…' if significant else 'â—‹'\n",
    "        color = 'gold' if significant else 'lightgray'\n",
    "        ax1.text(i - width/2, actual + 0.02, marker, ha='center', va='bottom', \n",
    "                fontsize=16, color=color, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (actual, expected) in enumerate(zip(actual_recalls, expected_recalls)):\n",
    "        ax1.text(i - width/2, actual + 0.01, f'{actual:.3f}', ha='center', va='bottom', \n",
    "                fontsize=9, fontweight='bold')\n",
    "        ax1.text(i + width/2, expected + 0.01, f'{expected:.3f}', ha='center', va='bottom', \n",
    "                fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Performance Improvement over Random\n",
    "    improvements = [actual - expected for actual, expected in zip(actual_recalls, expected_recalls)]\n",
    "    bar_colors = ['darkgreen' if imp > 0 else 'darkred' for imp in improvements]\n",
    "    \n",
    "    bars = ax2.bar(present_names, improvements, color=bar_colors, alpha=0.7, edgecolor='black')\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    ax2.set_title('Recall Improvement over Random Baseline', fontweight='bold')\n",
    "    ax2.set_ylabel('Recall Difference')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, imp, significant in zip(bars, improvements, is_significant):\n",
    "        height = bar.get_height()\n",
    "        va = 'bottom' if height >= 0 else 'top'\n",
    "        y_pos = height + (0.005 if height >= 0 else -0.005)\n",
    "        significance_marker = ' â˜…' if significant else ''\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., y_pos,\n",
    "                f'{imp:+.3f}{significance_marker}', ha='center', va=va, \n",
    "                fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Plot 3: Statistical Significance Summary\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    # Create significance summary\n",
    "    total_signals = len(present_signals)\n",
    "    significant_count = sum(is_significant)\n",
    "    \n",
    "    significance_text = f\"\"\"\n",
    "    ðŸŽ¯ STATISTICAL SIGNIFICANCE ANALYSIS\n",
    "    {'='*40}\n",
    "    Confidence Level: {confidence*100:.1f}%\n",
    "    \n",
    "    Significant Signals: {significant_count}/{total_signals}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for signal, name, significant, actual, expected, ci_lower, ci_upper in zip(\n",
    "        present_signals, present_names, is_significant, actual_recalls, expected_recalls, ci_lowers, ci_uppers):\n",
    "        \n",
    "        status = \"âœ… SIGNIFICANT\" if significant else \"âŒ Not Significant\"\n",
    "        significance_text += f\"\"\"\n",
    "    {name}:\n",
    "    Actual Recall: {actual:.4f}\n",
    "    Expected: {expected:.4f} [{ci_lower:.4f}, {ci_upper:.4f}]\n",
    "    Status: {status}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax3.text(0.05, 0.95, significance_text, transform=ax3.transAxes, fontsize=10,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    # Plot 4: Signal Distribution and Recall Summary\n",
    "    total_samples = sum(signal_counts.values())\n",
    "    present_counts = [signal_counts[signal] for signal in present_signals]\n",
    "    \n",
    "    # Create pie chart for signal distribution\n",
    "    wedges, texts, autotexts = ax4.pie(present_counts, \n",
    "                                      labels=[f'{name}\\n({count:,})' for name, count in zip(present_names, present_counts)], \n",
    "                                      colors=present_colors, \n",
    "                                      autopct='%1.2f%%', \n",
    "                                      startangle=90)\n",
    "    ax4.set_title('Signal Distribution with Recall Values', fontweight='bold')\n",
    "    \n",
    "    # Add recall values as text around the pie\n",
    "    for i, (signal, recall_val) in enumerate(zip(present_signals, actual_recalls)):\n",
    "        ax4.text(0.1, 0.9 - i*0.15, f'Signal {signal}: {recall_val:.3f}', \n",
    "                transform=ax4.transAxes, fontsize=10, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round', facecolor=present_colors[i], alpha=0.3))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Keep original function for backward compatibility\n",
    "def plot_prediction_performance(predictions: pd.Series, targets: pd.Series, \n",
    "                              confidence: float = 0.95, \n",
    "                              figsize: Tuple[int, int] = (15, 10),\n",
    "                              title_prefix: str = \"\") -> plt.Figure:\n",
    "    \"\"\"\n",
    "    ðŸŽ¯ COMPREHENSIVE PERFORMANCE VISUALIZATION (ORIGINAL)\n",
    "    \n",
    "    Visualize prediction performance against random baseline with significance testing.\n",
    "    \n",
    "    Args:\n",
    "        predictions (pd.Series): Predicted values\n",
    "        targets (pd.Series): True target values\n",
    "        confidence (float): Confidence level for significance testing\n",
    "        figsize (tuple): Figure size (width, height)\n",
    "        title_prefix (str): Prefix for the plot title\n",
    "        \n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib figure object with 4 subplots\n",
    "    \"\"\"\n",
    "    # Get significance results\n",
    "    significance_results = prediction_recall_significance(predictions, targets, confidence)\n",
    "    baseline_results = compute_all_recall_intervals_random_baseline(targets, confidence)\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=figsize)\n",
    "    fig.suptitle(f'{title_prefix}Prediction Performance vs Random Baseline', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Prepare data\n",
    "    signals = [-1, 0, 1]\n",
    "    signal_names = ['Sell (-1)', 'Hold (0)', 'Buy (1)']\n",
    "    colors = ['red', 'gray', 'green']\n",
    "    \n",
    "    # Filter for present signals\n",
    "    present_signals = [s for s in signals if s in significance_results]\n",
    "    present_names = [signal_names[signals.index(s)] for s in present_signals]\n",
    "    present_colors = [colors[signals.index(s)] for s in present_signals]\n",
    "    \n",
    "    actual_recalls = [significance_results[signal]['recall'] for signal in present_signals]\n",
    "    expected_recalls = [significance_results[signal]['expected_random'] for signal in present_signals]\n",
    "    ci_lowers = [significance_results[signal]['ci_lower'] for signal in present_signals]\n",
    "    ci_uppers = [significance_results[signal]['ci_upper'] for signal in present_signals]\n",
    "    is_significant = [significance_results[signal]['significant'] for signal in present_signals]\n",
    "    \n",
    "    # Plot 1: Actual vs Expected Recall Comparison\n",
    "    x_pos = np.arange(len(present_signals))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax1.bar(x_pos - width/2, actual_recalls, width, label='Actual Recall', \n",
    "                    color=present_colors, alpha=0.8, edgecolor='black')\n",
    "    bars2 = ax1.bar(x_pos + width/2, expected_recalls, width, label='Expected (Random)', \n",
    "                    color=present_colors, alpha=0.4, edgecolor='black', hatch='///')\n",
    "    \n",
    "    # Add confidence intervals for expected recalls\n",
    "    ax1.errorbar(x_pos + width/2, expected_recalls, \n",
    "                yerr=[np.array(expected_recalls) - np.array(ci_lowers),\n",
    "                      np.array(ci_uppers) - np.array(expected_recalls)],\n",
    "                fmt='none', color='black', capsize=3, capthick=1)\n",
    "    \n",
    "    ax1.set_title('Actual vs Expected Recall', fontweight='bold')\n",
    "    ax1.set_ylabel('Recall')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(present_names)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add significance markers\n",
    "    for i, (actual, significant) in enumerate(zip(actual_recalls, is_significant)):\n",
    "        marker = 'â˜…' if significant else 'â—‹'\n",
    "        color = 'gold' if significant else 'lightgray'\n",
    "        ax1.text(i - width/2, actual + 0.02, marker, ha='center', va='bottom', \n",
    "                fontsize=16, color=color, fontweight='bold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (actual, expected) in enumerate(zip(actual_recalls, expected_recalls)):\n",
    "        ax1.text(i - width/2, actual + 0.01, f'{actual:.3f}', ha='center', va='bottom', \n",
    "                fontsize=9, fontweight='bold')\n",
    "        ax1.text(i + width/2, expected + 0.01, f'{expected:.3f}', ha='center', va='bottom', \n",
    "                fontsize=9, fontweight='bold')\n",
    "    \n",
    "    # Plot 2: Performance Improvement over Random\n",
    "    improvements = [actual - expected for actual, expected in zip(actual_recalls, expected_recalls)]\n",
    "    bar_colors = ['darkgreen' if imp > 0 else 'darkred' for imp in improvements]\n",
    "    \n",
    "    bars = ax2.bar(present_names, improvements, color=bar_colors, alpha=0.7, edgecolor='black')\n",
    "    ax2.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    ax2.set_title('Recall Improvement over Random Baseline', fontweight='bold')\n",
    "    ax2.set_ylabel('Recall Difference')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, imp, significant in zip(bars, improvements, is_significant):\n",
    "        height = bar.get_height()\n",
    "        va = 'bottom' if height >= 0 else 'top'\n",
    "        y_pos = height + (0.005 if height >= 0 else -0.005)\n",
    "        significance_marker = ' â˜…' if significant else ''\n",
    "        ax2.text(bar.get_x() + bar.get_width()/2., y_pos,\n",
    "                f'{imp:+.3f}{significance_marker}', ha='center', va=va, \n",
    "                fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Plot 3: Statistical Significance Summary\n",
    "    ax3.axis('off')\n",
    "    \n",
    "    # Create significance summary\n",
    "    total_signals = len(present_signals)\n",
    "    significant_count = sum(is_significant)\n",
    "    \n",
    "    significance_text = f\"\"\"\n",
    "    ðŸŽ¯ STATISTICAL SIGNIFICANCE ANALYSIS\n",
    "    {'='*40}\n",
    "    Confidence Level: {confidence*100:.1f}%\n",
    "    \n",
    "    Significant Signals: {significant_count}/{total_signals}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    for signal, name, significant, actual, expected, ci_lower, ci_upper in zip(\n",
    "        present_signals, present_names, is_significant, actual_recalls, expected_recalls, ci_lowers, ci_uppers):\n",
    "        \n",
    "        status = \"âœ… SIGNIFICANT\" if significant else \"âŒ Not Significant\"\n",
    "        significance_text += f\"\"\"\n",
    "    {name}:\n",
    "    Actual Recall: {actual:.4f}\n",
    "    Expected: {expected:.4f} [{ci_lower:.4f}, {ci_upper:.4f}]\n",
    "    Status: {status}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax3.text(0.05, 0.95, significance_text, transform=ax3.transAxes, fontsize=10,\n",
    "             verticalalignment='top', fontfamily='monospace',\n",
    "             bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))\n",
    "    \n",
    "    # Plot 4: Prediction vs Target Distribution\n",
    "    pred_counts = predictions.value_counts().reindex(present_signals, fill_value=0)\n",
    "    target_counts = targets.value_counts().reindex(present_signals, fill_value=0)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_data = pd.DataFrame({\n",
    "        'Predictions': pred_counts.values,\n",
    "        'Targets': target_counts.values\n",
    "    }, index=present_names)\n",
    "    \n",
    "    comparison_data.plot(kind='bar', ax=ax4, color=['lightblue', 'orange'], alpha=0.7)\n",
    "    ax4.set_title('Prediction vs Target Distribution', fontweight='bold')\n",
    "    ax4.set_ylabel('Count')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"ðŸŽ¯ ENHANCED PERFORMANCE VISUALIZATION FUNCTIONS DEFINED\")\n",
    "print(\"ðŸ“Š Use plot_prediction_performance_from_recalls() for direct recall analysis\")\n",
    "print(\"âœ… Creates 4-panel analysis: actual vs expected, improvement, significance, distribution\")\n",
    "print(\"â­ Shows significance with star markers\")\n",
    "print(\"ðŸ”„ Original function preserved for backward compatibility\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f227f",
   "metadata": {},
   "source": [
    "## 6. Theoretical Validation Functions\n",
    "\n",
    "Validate our analytical confidence intervals using Monte Carlo simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f0b24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theoretical_recall_distribution(targets: pd.Series, signal: int, n_simulations: int = 10000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulate the theoretical distribution of recall under random baseline prediction.\n",
    "    \n",
    "    This function validates our analytical confidence intervals by simulation.\n",
    "    \n",
    "    Args:\n",
    "        targets (pd.Series): True target values\n",
    "        signal (int): The signal value to calculate recall distribution for\n",
    "        n_simulations (int): Number of simulations to run\n",
    "        \n",
    "    Returns:\n",
    "        np.ndarray: Array of recall values from simulations\n",
    "    \"\"\"\n",
    "    total_samples = len(targets)\n",
    "    actual_positives = np.sum(targets == signal)\n",
    "    \n",
    "    if actual_positives == 0:\n",
    "        return np.array([0.0] * n_simulations)\n",
    "    \n",
    "    # Get unique values and their proportions\n",
    "    unique_values = targets.unique()\n",
    "    proportions = [np.sum(targets == val) / total_samples for val in unique_values]\n",
    "    \n",
    "    recall_values = []\n",
    "    \n",
    "    for _ in range(n_simulations):\n",
    "        # Generate random predictions following target distribution\n",
    "        random_predictions = np.random.choice(\n",
    "            unique_values, \n",
    "            size=total_samples, \n",
    "            p=proportions\n",
    "        )\n",
    "        \n",
    "        # Calculate recall for this simulation\n",
    "        true_positives = np.sum((random_predictions == signal) & (targets == signal))\n",
    "        recall_sim = true_positives / actual_positives\n",
    "        recall_values.append(recall_sim)\n",
    "    \n",
    "    return np.array(recall_values)\n",
    "\n",
    "print(\"ðŸ”¬ Theoretical validation function defined\")\n",
    "print(\"âœ… Simulates random baseline to validate analytical confidence intervals\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95c7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_theoretical_validation(targets: pd.Series, confidence: float = 0.95, \n",
    "                              n_simulations: int = 10000,\n",
    "                              figsize: Tuple[int, int] = (15, 5)) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Validate analytical confidence intervals against theoretical simulation.\n",
    "    \n",
    "    Args:\n",
    "        targets (pd.Series): True target values\n",
    "        confidence (float): Confidence level\n",
    "        n_simulations (int): Number of simulations for validation\n",
    "        figsize (tuple): Figure size\n",
    "        \n",
    "    Returns:\n",
    "        plt.Figure: The matplotlib figure object\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "    fig.suptitle(f'Theoretical Validation: Analytical CI vs Simulation ({n_simulations:,} runs)', \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    signals = [-1, 0, 1]\n",
    "    signal_names = ['Sell (-1)', 'Hold (0)', 'Buy (1)']\n",
    "    colors = ['red', 'gray', 'green']\n",
    "    \n",
    "    baseline_results = compute_all_recall_intervals_random_baseline(targets, confidence)\n",
    "    \n",
    "    plot_idx = 0\n",
    "    for signal, name, color in zip(signals, signal_names, colors):\n",
    "        if signal not in targets.values or plot_idx >= len(axes):\n",
    "            if plot_idx < len(axes):\n",
    "                axes[plot_idx].text(0.5, 0.5, f'Signal {signal}\\nnot present', \n",
    "                            ha='center', va='center', transform=axes[plot_idx].transAxes)\n",
    "                axes[plot_idx].set_title(name)\n",
    "                plot_idx += 1\n",
    "            continue\n",
    "            \n",
    "        # Get analytical results\n",
    "        expected_recall = baseline_results[signal]['expected_recall']\n",
    "        ci_lower = baseline_results[signal]['ci_lower']\n",
    "        ci_upper = baseline_results[signal]['ci_upper']\n",
    "        \n",
    "        # Run simulation\n",
    "        simulated_recalls = theoretical_recall_distribution(targets, signal, n_simulations)\n",
    "        \n",
    "        # Plot histogram of simulated recalls\n",
    "        axes[plot_idx].hist(simulated_recalls, bins=50, alpha=0.7, color=color, \n",
    "                    density=True, edgecolor='black', linewidth=0.5)\n",
    "        \n",
    "        # Add analytical expected value and CI\n",
    "        axes[plot_idx].axvline(expected_recall, color='red', linestyle='--', linewidth=2, \n",
    "                       label=f'Analytical Expected: {expected_recall:.3f}')\n",
    "        axes[plot_idx].axvline(ci_lower, color='orange', linestyle=':', linewidth=2, \n",
    "                       label=f'Analytical CI: [{ci_lower:.3f}, {ci_upper:.3f}]')\n",
    "        axes[plot_idx].axvline(ci_upper, color='orange', linestyle=':', linewidth=2)\n",
    "        \n",
    "        # Add simulation statistics\n",
    "        sim_mean = np.mean(simulated_recalls)\n",
    "        \n",
    "        axes[plot_idx].axvline(sim_mean, color='blue', linestyle='-', linewidth=2, \n",
    "                       label=f'Simulation Mean: {sim_mean:.3f}')\n",
    "        \n",
    "        axes[plot_idx].set_title(name, fontweight='bold')\n",
    "        axes[plot_idx].set_xlabel('Recall')\n",
    "        axes[plot_idx].set_ylabel('Density')\n",
    "        axes[plot_idx].legend(fontsize=8)\n",
    "        axes[plot_idx].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add validation text\n",
    "        validation_text = f\"\"\"\n",
    "        Analytical: {expected_recall:.4f}\n",
    "        Simulation: {sim_mean:.4f}\n",
    "        Difference: {abs(expected_recall - sim_mean):.4f}\n",
    "        \"\"\"\n",
    "        axes[plot_idx].text(0.02, 0.98, validation_text, transform=axes[plot_idx].transAxes, \n",
    "                    verticalalignment='top', fontsize=8, fontfamily='monospace',\n",
    "                    bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "        \n",
    "        plot_idx += 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "print(\"ðŸ”¬ Theoretical validation visualization defined\")\n",
    "print(\"âœ… Compares analytical methods with Monte Carlo simulation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663ab3c1",
   "metadata": {},
   "source": [
    "## 7. Example Usage and Testing\n",
    "\n",
    "Let's create some sample data to demonstrate the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdf3af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data for demonstration\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Create sample targets with realistic class imbalance\n",
    "n_samples = 10000\n",
    "targets_sample = np.random.choice([-1, 0, 1], size=n_samples, p=[0.3, 0.4, 0.3])\n",
    "targets_sample = pd.Series(targets_sample, name='targets')\n",
    "\n",
    "# Create sample predictions (slightly better than random for demonstration)\n",
    "predictions_sample = targets_sample.copy()\n",
    "# Add some noise to make it realistic\n",
    "noise_indices = np.random.choice(n_samples, size=int(0.7 * n_samples), replace=False)\n",
    "predictions_sample.iloc[noise_indices] = np.random.choice([-1, 0, 1], size=len(noise_indices), p=[0.3, 0.4, 0.3])\n",
    "predictions_sample.name = 'predictions'\n",
    "\n",
    "print(\"ðŸ“Š Sample Data Generated:\")\n",
    "print(f\"Total samples: {len(targets_sample):,}\")\n",
    "print(f\"Target distribution:\")\n",
    "print(targets_sample.value_counts().sort_index())\n",
    "print(f\"\\nPrediction distribution:\")\n",
    "print(predictions_sample.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2bce17",
   "metadata": {},
   "source": [
    "### 7.1 Calculate Random Baseline Confidence Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bcc63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate baseline confidence intervals with verbose output\n",
    "baseline_results = compute_all_recall_intervals_random_baseline(targets_sample, confidence=0.95, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c2ee9b",
   "metadata": {},
   "source": [
    "### 7.2 Perform Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac4392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ MAIN SIGNIFICANCE TESTING\n",
    "significance_results = prediction_recall_significance(predictions_sample, targets_sample, confidence=0.95)\n",
    "\n",
    "print(\"ðŸŽ¯ RECALL SIGNIFICANCE ANALYSIS RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "signal_names = {-1: \"Sell\", 0: \"Hold\", 1: \"Buy\"}\n",
    "\n",
    "for signal in [-1, 0, 1]:\n",
    "    if signal in significance_results:\n",
    "        result = significance_results[signal]\n",
    "        signal_name = signal_names[signal]\n",
    "        \n",
    "        print(f\"\\n{signal_name} Signal ({signal}):\")\n",
    "        print(f\"  Actual Recall: {result['recall']:.4f}\")\n",
    "        print(f\"  Random Expected: {result['expected_random']:.4f}\")\n",
    "        print(f\"  Random CI: [{result['ci_lower']:.4f}, {result['ci_upper']:.4f}]\")\n",
    "        print(f\"  Significant: {'âœ… YES' if result['significant'] else 'âŒ NO'}\")\n",
    "        print(f\"  Improvement: {result['recall'] - result['expected_random']:+.4f}\")\n",
    "\n",
    "# Summary\n",
    "total_signals = len(significance_results)\n",
    "significant_signals = sum(result['significant'] for result in significance_results.values())\n",
    "print(f\"\\nðŸ“Š SUMMARY: {significant_signals}/{total_signals} signals are statistically significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08895e14",
   "metadata": {},
   "source": [
    "### 7.3 Visualize Random Baseline Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4181d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline confidence intervals visualization\n",
    "fig1 = plot_recall_confidence_intervals(targets_sample, confidence=0.95, \n",
    "                                       title_prefix=\"Sample Data: \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2949940b",
   "metadata": {},
   "source": [
    "### 7.4 Comprehensive Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e518de71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ COMPREHENSIVE PERFORMANCE VISUALIZATION\n",
    "fig2 = plot_prediction_performance(predictions_sample, targets_sample, \n",
    "                                  confidence=0.95, \n",
    "                                  title_prefix=\"Sample Model: \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecb79e8",
   "metadata": {},
   "source": [
    "### 7.5 Theoretical Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e741f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate analytical methods with simulation\n",
    "fig3 = plot_theoretical_validation(targets_sample, confidence=0.95, n_simulations=5000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564ac545",
   "metadata": {},
   "source": [
    "## 8. Direct Recall Analysis (NEW)\n",
    "\n",
    "Demonstrate the new functionality that works with recall values directly instead of computing from predictions/targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db6492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Working with Direct Recall Values\n",
    "# This is useful when you already have computed recall values from your model\n",
    "\n",
    "# Sample recall values (these could come from your model evaluation)\n",
    "sample_recall_values = {\n",
    "    -1: 0.65,  # Sell signal recall: 65%\n",
    "     0: 0.42,  # Hold signal recall: 42% \n",
    "     1: 0.68   # Buy signal recall: 68%\n",
    "}\n",
    "\n",
    "# Sample signal counts (how many times each signal appeared in your data)\n",
    "sample_signal_counts = {\n",
    "    -1: 3000,  # 3000 sell signals in dataset\n",
    "     0: 4000,  # 4000 hold signals in dataset\n",
    "     1: 3000   # 3000 buy signals in dataset\n",
    "}\n",
    "\n",
    "print(\"ðŸ†• DIRECT RECALL ANALYSIS EXAMPLE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Recall Values: {sample_recall_values}\")\n",
    "print(f\"Signal Counts: {sample_signal_counts}\")\n",
    "print(f\"Total Samples: {sum(sample_signal_counts.values()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3119800",
   "metadata": {},
   "source": [
    "### 8.1 Baseline Analysis with Direct Recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2834d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate baseline confidence intervals using recall values directly\n",
    "baseline_results_from_recalls = compute_all_recall_intervals_random_baseline_from_recalls(\n",
    "    sample_recall_values, sample_signal_counts, confidence=0.95, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be67409",
   "metadata": {},
   "source": [
    "### 8.2 Significance Testing with Direct Recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adf6415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ SIGNIFICANCE TESTING WITH DIRECT RECALL VALUES\n",
    "significance_results_from_recalls = prediction_recall_significance_from_recalls(\n",
    "    sample_recall_values, sample_signal_counts, confidence=0.95\n",
    ")\n",
    "\n",
    "print(\"ðŸŽ¯ RECALL SIGNIFICANCE ANALYSIS RESULTS (FROM DIRECT RECALLS)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "signal_names = {-1: \"Sell\", 0: \"Hold\", 1: \"Buy\"}\n",
    "\n",
    "for signal in [-1, 0, 1]:\n",
    "    if signal in significance_results_from_recalls:\n",
    "        result = significance_results_from_recalls[signal]\n",
    "        signal_name = signal_names[signal]\n",
    "        \n",
    "        print(f\"\\n{signal_name} Signal ({signal}):\")\n",
    "        print(f\"  Actual Recall: {result['recall']:.4f}\")\n",
    "        print(f\"  Random Expected: {result['expected_random']:.4f}\")\n",
    "        print(f\"  Random CI: [{result['ci_lower']:.4f}, {result['ci_upper']:.4f}]\")\n",
    "        print(f\"  Significant: {'âœ… YES' if result['significant'] else 'âŒ NO'}\")\n",
    "        print(f\"  Improvement: {result['recall'] - result['expected_random']:+.4f}\")\n",
    "\n",
    "# Summary\n",
    "total_signals = len(significance_results_from_recalls)\n",
    "significant_signals = sum(result['significant'] for result in significance_results_from_recalls.values())\n",
    "print(f\"\\nðŸ“Š SUMMARY: {significant_signals}/{total_signals} signals are statistically significant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04f077f",
   "metadata": {},
   "source": [
    "### 8.3 Comprehensive Visualization with Direct Recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb612acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸŽ¯ COMPREHENSIVE PERFORMANCE VISUALIZATION WITH DIRECT RECALLS\n",
    "fig_recalls = plot_prediction_performance_from_recalls(\n",
    "    sample_recall_values, sample_signal_counts, \n",
    "    confidence=0.95, \n",
    "    title_prefix=\"Direct Recall Analysis: \"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866f5538",
   "metadata": {},
   "source": [
    "### 8.4 Comparison: Direct Recalls vs Traditional Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c76ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's verify that both methods give the same results\n",
    "# by comparing with the traditional prediction/target approach\n",
    "\n",
    "print(\"ðŸ”„ COMPARISON: Direct Recalls vs Traditional Method\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare significance results\n",
    "print(\"\\nðŸŽ¯ Significance Testing Results:\")\n",
    "print(\"Signal | Direct Recalls | Traditional | Match\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for signal in [-1, 0, 1]:\n",
    "    if signal in significance_results and signal in significance_results_from_recalls:\n",
    "        direct_sig = significance_results_from_recalls[signal]['significant']\n",
    "        traditional_sig = significance_results[signal]['significant']\n",
    "        match = \"âœ…\" if direct_sig == traditional_sig else \"âŒ\"\n",
    "        \n",
    "        print(f\"{signal:6} | {direct_sig:13} | {traditional_sig:11} | {match}\")\n",
    "\n",
    "# Compare recall values\n",
    "print(\"\\nðŸ“Š Recall Values Comparison:\")\n",
    "print(\"Signal | Direct Recalls | Traditional | Difference\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for signal in [-1, 0, 1]:\n",
    "    if signal in significance_results and signal in significance_results_from_recalls:\n",
    "        direct_recall = significance_results_from_recalls[signal]['recall']\n",
    "        traditional_recall = significance_results[signal]['recall']\n",
    "        diff = abs(direct_recall - traditional_recall)\n",
    "        \n",
    "        print(f\"{signal:6} | {direct_recall:13.4f} | {traditional_recall:11.4f} | {diff:10.6f}\")\n",
    "\n",
    "print(\"\\nâœ… Both methods should give identical results when using the same underlying data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb2b5a8",
   "metadata": {},
   "source": [
    "## 9. Quick Reference Guide\n",
    "\n",
    "### ðŸŽ¯ Main Functions for Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d6a30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ¯ QUICK REFERENCE GUIDE\")\n",
    "print(\"=\" * 50)\n",
    "print()\n",
    "print(\"ðŸ“Š WORKING WITH DIRECT RECALL VALUES:\")\n",
    "print(\"   # Example recall values and signal counts\")\n",
    "print(\"   recall_values = {-1: 0.65, 0: 0.42, 1: 0.68}\")\n",
    "print(\"   signal_counts = {-1: 3000, 0: 4000, 1: 3000}\")\n",
    "print()\n",
    "print(\"   # Significance testing with recalls\")\n",
    "print(\"   results = prediction_recall_significance_from_recalls(recall_values, signal_counts)\")\n",
    "print()\n",
    "print(\"   # Visualization with recalls\")\n",
    "print(\"   fig = plot_prediction_performance_from_recalls(recall_values, signal_counts)\")\n",
    "print()\n",
    "print(\"ðŸ“Š TRADITIONAL PREDICTION/TARGET ANALYSIS:\")\n",
    "print(\"   results = prediction_recall_significance(predictions, targets)\")\n",
    "print(\"   fig = plot_prediction_performance(predictions, targets, confidence=0.95)\")\n",
    "print()\n",
    "print(\"ðŸ“Š BASELINE ANALYSIS:\")\n",
    "print(\"   baseline_results = compute_all_recall_intervals_random_baseline(targets, verbose=True)\")\n",
    "print()\n",
    "print(\"ðŸ”¬ THEORETICAL VALIDATION:\")\n",
    "print(\"   fig = plot_theoretical_validation(targets, confidence=0.95, n_simulations=10000)\")\n",
    "print()\n",
    "print(\"âœ… INTERPRETATION:\")\n",
    "print(\"   - significant=True: Model beats random baseline\")\n",
    "print(\"   - Stars (â˜…) indicate statistical significance\")\n",
    "print(\"   - Confidence intervals show random baseline range\")\n",
    "print()\n",
    "print(\"ðŸ†• NEW FEATURES:\")\n",
    "print(\"   - Functions ending with '_from_recalls' work with direct recall values\")\n",
    "print(\"   - Original functions preserved for backward compatibility\")\n",
    "print(\"   - Enhanced analysis supports both approaches\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c132fd",
   "metadata": {},
   "source": [
    "## 10. Load Your Own Data\n",
    "\n",
    "Use this template to analyze your own model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfb845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for your own analysis\n",
    "print(\"ðŸ“‹ TEMPLATE FOR YOUR ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "print()\n",
    "print(\"# Option 1: Working with direct recall values\")\n",
    "print(\"# recall_values = {-1: 0.65, 0: 0.42, 1: 0.68}  # Your actual recall values\")\n",
    "print(\"# signal_counts = {-1: 3000, 0: 4000, 1: 3000}   # Count of each signal in your data\")\n",
    "print()\n",
    "print(\"# results = prediction_recall_significance_from_recalls(recall_values, signal_counts)\")\n",
    "print(\"# fig = plot_prediction_performance_from_recalls(recall_values, signal_counts,\")\n",
    "print(\"#                                                 title_prefix='My Model: ')\")\n",
    "print(\"# plt.show()\")\n",
    "print()\n",
    "print(\"# Option 2: Traditional approach with predictions/targets\")\n",
    "print(\"# your_predictions = pd.Series([...])  # Your model predictions (-1, 0, 1)\")\n",
    "print(\"# your_targets = pd.Series([...])      # True target values (-1, 0, 1)\")\n",
    "print()\n",
    "print(\"# results = prediction_recall_significance(your_predictions, your_targets)\")\n",
    "print(\"# fig = plot_prediction_performance(your_predictions, your_targets,\")\n",
    "print(\"#                                   title_prefix='My Model: ')\")\n",
    "print(\"# plt.show()\")\n",
    "print()\n",
    "print(\"# Print summary for both approaches\")\n",
    "print(\"# for signal, result in results.items():\")\n",
    "print(\"#     print(f\\\"Signal {signal}: {'Significant' if result['significant'] else 'Not Significant'}\\\")\")\n",
    "print()\n",
    "print(\"ðŸ†• Choose the approach that best fits your data:\")\n",
    "print(\"   - Use '_from_recalls' functions if you already have recall values\")\n",
    "print(\"   - Use original functions if you have prediction/target series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4acce3",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "This notebook provides a complete framework for:\n",
    "\n",
    "### âœ… **Core Capabilities**\n",
    "- **Statistical Significance Testing**: Determine if your model beats random baseline\n",
    "- **Confidence Intervals**: Wilson method for robust statistical inference\n",
    "- **Comprehensive Visualizations**: 4-panel analysis plots\n",
    "- **Theoretical Validation**: Monte Carlo simulation verification\n",
    "\n",
    "### ðŸŽ¯ **Key Functions**\n",
    "1. **`prediction_recall_significance()`** - Main significance testing function\n",
    "2. **`plot_prediction_performance()`** - Comprehensive visualization\n",
    "3. **`compute_all_recall_intervals_random_baseline()`** - Baseline analysis\n",
    "4. **`plot_theoretical_validation()`** - Simulation validation\n",
    "\n",
    "### ðŸ“Š **Output Interpretation**\n",
    "- **Stars (â˜…)**: Indicate statistical significance\n",
    "- **Confidence Intervals**: Show random baseline performance range\n",
    "- **Improvement Bars**: Show how much better (or worse) than random\n",
    "- **Significance Summary**: Detailed statistical results\n",
    "\n",
    "### ðŸš€ **Next Steps**\n",
    "1. Load your model predictions and targets\n",
    "2. Run `prediction_recall_significance()` for main analysis\n",
    "3. Use `plot_prediction_performance()` for visualization\n",
    "4. Interpret results using significance markers and confidence intervals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
